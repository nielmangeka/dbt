# ğŸ§  dbt-learn

## ğŸš€ Introduction

Welcome to **dbt-learn** â€” a mini project created to make productive use of holiday time while diving deeper into modern data tools, especially those commonly used in the data engineering and analytics space.

This project focuses on exploring **DBT (Data Build Tool)** and **PostgreSQL**, aiming to simulate a simple data warehousing flow using modern data transformation practices. Itâ€™s also a personal learning journey Iâ€™ve long wanted to embark on â€” and now finally had the time to start!

Any feedback, suggestions, or ideas are very welcome ğŸ™Œ

---

## âš™ï¸ Prerequisites

### ğŸ”§ Tools Used

* **DBT Core**
* **PostgreSQL**

---

## ğŸ“¥ Installation

### 1. Install DBT

You can follow the official documentation for installation guidance. In this project, I used **DBT Core with the Postgres adapter**, but you can choose the adapter that fits your own data warehouse setup.

ğŸ”— [DBT Installation Guide](https://docs.getdbt.com/docs/core/pip-install#installing-the-adapter)

---

## ğŸ—‚ï¸ Project Architecture

This project simulates a basic data pipeline and warehousing flow using **PostgreSQL** and **dbt**:

1. **Raw Layer**
   Raw data is stored in a schema called `raw`.

2. **Staging Layer**
   The raw data is cleaned and standardized using **dbt models**, then loaded into staging tables (dimension and fact tables).

3. **Data Mart**
   Final datasets (data marts) are created from the staging layer â€” ready for analytics and reporting.

> All transformations are written and managed using **dbt**, making the pipeline modular, testable, and production-friendly.

### ğŸ—ºï¸ Data Flow Diagram

![Data Flow Diagram](images.png)

---

## ğŸ“« Feedback

If you have ideas to improve this project, found something unclear, or simply want to connect â€” feel free to open an issue or message me. This is a learning project, and collaboration is always welcome!

---

Let me know if you want to add more sections like:

* How to run the project
* Example queries or outputs
* DBT model directory structure
* Dataset sources

I'd be happy to help tailor this further.
